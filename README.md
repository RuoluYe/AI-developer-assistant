# GPT Evaluator

This repository contains a FastAPI application that allows users to input a text prompt and receive an evaluated response generated by OpenAI's GPT model. The response is evaluated based on relevance, coherence, consistency, and fluency.

## Features
- OpenAI GPT-4 model integration for text generation.
- Evaluation of the generated text on predefined metrics.

## Prerequisites

Before you begin, ensure you have met the following requirements:

- You have installed Python 3.6+.
- You have a working installation of `pip`.
- You have an OpenAI API key.

## Installation

Before installing the required packages, it is recommended to create a virtual environment:

### Creating a Virtual Environment

On macOS and Linux:

```bash
python3 -m venv venv
```

On Windows:

```bash
python -m venv venv
```

### Activating the Virtual Environment

Activate the virtual environment before installing dependencies and running the application.

On macOS and Linux:

```bash
source venv/bin/activate
```

On Windows:

```bash
.\venv\Scripts\activate
```

### Install Dependencies

With the virtual environment activated, install the necessary packages using:

```bash
pip install -r requirements.txt
```

## Setting Up Your Environment

1. Clone the repository to your local machine.
2. Navigate to the repository directory.
3. If you haven't already, activate your virtual environment.
4. Create a `.env` file in the root of the project to store your OpenAI API key securely:

```dotenv
# .env file
OPENAI_API_KEY='your-api-key-here'
```

## Running the Application

With the virtual environment activated and the environment variables set, start the application using:

```bash
python app.py
```

This will start the FastAPI server on `http://127.0.0.1:8000`. The application is configured to automatically reload upon any code changes, facilitating development.

## Usage

Once the server is running:

1. Open your web browser and go to `http://127.0.0.1:8000`.
2. Enter your text into the text area provided on the page.
3. Click the "Evaluate" button to submit the text.
4. The evaluated response and scores will be displayed on the page.

## API Reference

### POST `/generate-and-score/`

Submit a text prompt for evaluation.

- Request Body: `{ "prompt": "Your text here" }`
- Response: A JSON object with the original prompt, generated response, and evaluation scores.

## Customization

You can customize the evaluation criteria and scoring logic by modifying the `evaluation_utils/eval_criteria.py` file.

## Contributing

Contributions to this project are welcome! Feel free to fork the repository and submit pull requests.

## License

Distributed under the MIT License. See `LICENSE` for more information.